# TinyLlama: "Attention Is All You Need" Inspection Lab ğŸ”¬

ì´ í”„ë¡œì íŠ¸ëŠ” 2017ë…„ Vaswani et al.ì´ ë°œí‘œí•œ ë…¼ë¬¸ **["Attention Is All You Need"](https://arxiv.org/abs/1706.03762)**ì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ì¸ **Transformer**ì˜ ë™ì‘ ì›ë¦¬ë¥¼, **TinyLlama-1.1B** ëª¨ë¸ì„ í†µí•´ ì‹œê°ì ìœ¼ë¡œ ê²€ì¦í•˜ê³  ë¶„ì„í•˜ëŠ” ì—°êµ¬ì†Œì…ë‹ˆë‹¤.

## ğŸ¯ Project Goal & Motivation
- **Why TinyLlama?**: ë…¼ë¬¸ì˜ Transformer êµ¬ì¡° ì¤‘ **Decoder-only** ì•„í‚¤í…ì²˜(GPT ê³„ì—´)ë¥¼ ë”°ë¥´ë©´ì„œë„, RTX 2060(6GB) í™˜ê²½ì—ì„œ ì „ì²´ ë ˆì´ì–´ ë¶„ì„ì´ ê°€ëŠ¥í•œ ìµœì ì˜ ëª¨ë¸ì…ë‹ˆë‹¤.
- **Core Mission**: ë¸”ë™ë°•ìŠ¤ì²˜ëŸ¼ ì—¬ê²¨ì§€ëŠ” LLM ë‚´ë¶€ì˜ **Query, Key, Value ($Q, K, V$)** ìƒí˜¸ì‘ìš©ì„ ì‹œê°í™”í•˜ì—¬, ë…¼ë¬¸ì˜ ìˆ˜ì‹ì´ ì‹¤ì œ ì½”ë“œì—ì„œ ì–´ë–»ê²Œ ë°œí˜„ë˜ëŠ”ì§€ ì¦ëª…í•©ë‹ˆë‹¤.

## ğŸ› ï¸ Environment Setup
- **GPU**: NVIDIA GeForce RTX 2060 (6GB VRAM) - *CUDA Core ê°€ì† í™œì„±í™”*
- **Container**: Docker (WSL2)
- **Base Image**: PyTorch 2.2.2 + CUDA 12.1
- **Library**: Hugging Face Transformers, Seaborn (Visualization)

## ğŸ“š Curriculum & Roadmap (Paper Mapping)

### Phase 1: Scaled Dot-Product Attention (Section 3.2.1)
- **ëª©í‘œ**: ë…¼ë¬¸ì˜ í•µì‹¬ ìˆ˜ì‹ $Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$ ì‹œê°í™” / (attention_matrixì— ë“¤ì–´ìˆëŠ” ê°’ë“¤(0.2555 ê°™ì€ ìˆ«ì))
- **í™œë™**: JupyterLabì—ì„œ í† í° ê°„ì˜ ì—°ê´€ì„±(Heatmap)ì„ ì¶”ì¶œí•˜ì—¬, ë‹¨ì–´ ì‚¬ì´ì˜ 'ìœ ì‚¬ë„'ê°€ ì–´ë–»ê²Œ ê³„ì‚°ë˜ëŠ”ì§€ í™•ì¸.

### Phase 2: Masked Multi-Head Attention (Section 3.2.2)
- **ëª©í‘œ**: ìƒì„±í˜• ëª¨ë¸(Decoder)ì˜ íŠ¹ì§•ì¸ **Causal Masking** ê²€ì¦.
- **í™œë™**: íˆíŠ¸ë§µì—ì„œ **ì‚¼ê°í˜• íŒ¨í„´(Upper Triangular Mask)**ì´ ë‚˜íƒ€ë‚˜ëŠ” ì´ìœ  ë¶„ì„. (ë¯¸ë˜ì˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ ë³´ì§€ ëª»í•˜ê²Œ í•˜ëŠ” ì›ë¦¬ ê·œëª…)

### Phase 3: Multi-Head Dynamics
- **ëª©í‘œ**: "Multi-Head"ì˜ ì˜ë¯¸ ì´í•´.
- **í™œë™**: ì„œë¡œ ë‹¤ë¥¸ Head(ì˜ˆ: Head 0 vs Head 10)ê°€ ê°™ì€ ë¬¸ì¥ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ë²•ì /ì˜ë¯¸ì  ìš”ì†Œì— ì£¼ëª©í•˜ê³  ìˆìŒì„ ë¹„êµ ë¶„ì„.

---
*Created by [HyunKN] on Micro-AI Lab*
